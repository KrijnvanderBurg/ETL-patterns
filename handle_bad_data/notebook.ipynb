{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Healing Data Pipelines: Automatically Handle and Reprocess Bad Data\n",
    "\n",
    "This notebook is reference in:\n",
    "- https://krijnvanrooijen.nl/blog/self-healing-data-pipeline-handling-bad-data/\n",
    "- https://krijnvanderburg.medium.com/self-healing-data-pipeline-handling-bad-data-b95c28c5458b\n",
    "\n",
    "Encountering bad data is inevitable in data pipelines. Schema mismatches, missing values, and inconsistent formatting are common causes of failures. The question isn't *if* bad data will occur, but *how frequently* it happens. However, rather than halting the entire pipeline when encountering bad data, the goal is to handle it gracefully so that the pipeline can process all valid incoming data without disruption.\n",
    "\n",
    "This notebook demonstrates how to build resilient data pipelines that effectively manage bad data using PySpark. By implementing these strategies, pipelines can continue to run uninterrupted, minimizing downtime and issues.\n",
    "\n",
    "## Key Strategies\n",
    "\n",
    "1. **Isolate and store bad records** - Separate invalid data to process valid records without disruption\n",
    "2. **Automate reprocessing** - Attempt to reprocess previously failed records in subsequent pipeline runs\n",
    "3. **Maintain testing history** - Keep records of problematic data for future debugging and pipeline validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Schema Definition and Bad Data Handling Setup\n",
    "\n",
    "First, we define our schema with a special `_corrupt_record` field that captures malformed rows. We'll use PySpark's \"PERMISSIVE\" mode, which allows corrupt records to be captured rather than causing the entire pipeline to fail.\n",
    "\n",
    "The PERMISSIVE mode is crucial for self-healing pipelines as it allows records that don't match the schema to be isolated in the `_corrupt_record` column instead of failing the entire job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 08:37:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.Builder().appName(\"Self-Healing Pipeline\").getOrCreate()\n",
    "\n",
    "# Define schema for validation \n",
    "# The _corrupt_record field is a special column that captures malformed rows when using PERMISSIVE mode\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Next, we'll create test data from two sources to simulate a real-world pipeline scenario:\n",
    "\n",
    "1. **New incoming data** - Contains both valid records and records with schema violations\n",
    "2. **Previously invalid records** - Represents data that failed in previous pipeline runs and is being reprocessed\n",
    "\n",
    "This approach demonstrates how our pipeline can handle both new bad data and attempt to reprocess historical bad data in a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDDs for test data\n",
    "# In production, you would read from files instead of using test data\n",
    "new_data_rdd = spark.sparkContext.parallelize([\n",
    "    \"1,John Doe,30\",\n",
    "    \"2,Jane Smith,25\",\n",
    "    \"3,Bob Johnson,35\",\n",
    "    \"4,Alice Johnson,twenty\"  # Invalid: age is not an integer\n",
    "])\n",
    "\n",
    "# Previously invalid records to reprocess\n",
    "previous_invalid_rdd = spark.sparkContext.parallelize([\n",
    "    \"5,45,Charlie Brown\",  # Invalid: id and age are position swapped\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading with Schema Validation\n",
    "\n",
    "Now, we load the data applying our predefined schema. Any records that don't match the expected structure will be automatically flagged. The schema validation process:\n",
    "\n",
    "- Parses each row according to the defined schema\n",
    "- Places properly formatted data in the appropriate columns\n",
    "- Captures malformed rows in the `_corrupt_record` column for later handling\n",
    "- Allows the pipeline to continue processing valid records without interruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All records:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----+--------------------+\n",
      "| id|         name| age|     _corrupt_record|\n",
      "+---+-------------+----+--------------------+\n",
      "|  1|     John Doe|  30|                NULL|\n",
      "|  2|   Jane Smith|  25|                NULL|\n",
      "|  3|  Bob Johnson|  35|                NULL|\n",
      "|  4|Alice Johnson|NULL|4,Alice Johnson,t...|\n",
      "|  5|           45|NULL|  5,45,Charlie Brown|\n",
      "+---+-------------+----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read data with schema validation\n",
    "new_df = spark.read.schema(schema).csv(new_data_rdd)\n",
    "previous_invalid_df = spark.read.schema(schema).csv(previous_invalid_rdd)\n",
    "\n",
    "# Combine datasets for unified processing\n",
    "combined_df = new_df.union(previous_invalid_df)\n",
    "\n",
    "# Show all data including corrupt records\n",
    "print(\"All records:\")\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Separating Valid and Invalid Records\n",
    "\n",
    "With our data loaded, we can now separate valid records from invalid ones using the `_corrupt_record` column. This is the key step in isolating bad data for separate handling:\n",
    "\n",
    "- Records with NULL in the `_corrupt_record` column are valid and can be processed normally\n",
    "- Records with values in the `_corrupt_record` column contain the original malformed data and need special handling\n",
    "\n",
    "This separation allows the pipeline to process valid records while preserving problematic data for future reprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate valid from invalid records\n",
    "valid_df = combined_df.filter(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\")\n",
    "invalid_df = combined_df.filter(col(\"_corrupt_record\").isNotNull()).select(\"_corrupt_record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Analysis\n",
    "\n",
    "Let's examine the results of our data separation to confirm that:\n",
    "1. Valid records have been properly identified and are ready for further processing\n",
    "2. Invalid records have been isolated along with their original format for future debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid records:\n",
      "+---+-----------+---+\n",
      "| id|       name|age|\n",
      "+---+-----------+---+\n",
      "|  1|   John Doe| 30|\n",
      "|  2| Jane Smith| 25|\n",
      "|  3|Bob Johnson| 35|\n",
      "+---+-----------+---+\n",
      "\n",
      "Invalid records (original form):\n",
      "+--------------------+\n",
      "|     _corrupt_record|\n",
      "+--------------------+\n",
      "|4,Alice Johnson,t...|\n",
      "|  5,45,Charlie Brown|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "print(\"Valid records:\")\n",
    "valid_df.show()\n",
    "\n",
    "print(\"Invalid records (original form):\")\n",
    "invalid_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Writing Results to Storage\n",
    "\n",
    "The final step in our self-healing pipeline is to write both valid and invalid records to appropriate storage locations:\n",
    "\n",
    "1. **Valid Records**: Written to the primary data destination in a structured format with headers\n",
    "2. **Invalid Records**: Written to a separate location in their original form for later reprocessing\n",
    "\n",
    "This approach ensures data processing continuity while maintaining a complete record of all incoming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write valid records as CSV with headers\n",
    "valid_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"handle_bad_data/output/good\")\n",
    "\n",
    "# Write invalid records in their original form as text\n",
    "# Using text format because _corrupt_record contains the original CSV line as string\n",
    "invalid_df.write.option(\"header\", \"false\").mode(\"overwrite\").text(\"handle_bad_data/output/bad\")\n",
    "\n",
    "# Optionally, consider writing the bad data to a bad data history archive in append mode\n",
    "#   to maintain a permanent track of bad data for testing the pipeline.\n",
    "#   Not included in this archive code is deduplication; \n",
    "#   the same bad record might fail to be reprocessed multiple times.\n",
    "# invalid_df.write.option(\"header\", \"false\").mode(\"append\").text(\"handle_bad_data/output/bad_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Key Takeaways\n",
    "\n",
    "This notebook has demonstrated how to build a self-healing data pipeline that gracefully handles bad data using PySpark. By implementing these techniques, data engineers can create more resilient systems that continue functioning even when encountering problematic data.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Isolate and Manage Bad Data**: Keep bad records separate from valid data to prevent pipeline failures.\n",
    "\n",
    "2. **Automate Bad Data Reprocessing**: Automatically reprocess bad data when underlying issues are fixed, removing the need for manual intervention.\n",
    "\n",
    "3. **Track Bad Data History**: Retain historical records of bad data to ensure that all data can be tested and properly processed in future pipeline versions.\n",
    "\n",
    "By following these practices, data pipelines become more robust, capable of handling errors efficiently, and can evolve without being derailed by data quality issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
