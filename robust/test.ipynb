{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e22a520",
   "metadata": {},
   "source": [
    "# Robust pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc1665",
   "metadata": {},
   "source": [
    "| Requirement ID | Description                     | User Story                                                                 | Expected Behaviour / Outcome                                      |\n",
    "|-----------------|---------------------------------|---------------------------------------------------------------------------|-------------------------------------------------------------------|\n",
    "| RQ-001          | User Authentication            | As a user, I want to log in securely so that I can access my account.    | The system should validate credentials and grant access securely. |\n",
    "| RQ-002          | Data Upload                    | As a user, I want to upload files so that I can share them with others.  | The system should allow file uploads and provide confirmation.   |\n",
    "| RQ-003          | Data Visualization             | As a user, I want to view data charts so that I can analyze trends.      | The system should generate and display interactive charts.       |\n",
    "| RQ-004          | Notification System            | As a user, I want to receive alerts so that I stay informed.             | The system should send timely notifications via email or SMS.    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2483c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"hour\", IntegerType(), nullable=False),\n",
    "    StructField(\"temperature\", IntegerType(), nullable=False),\n",
    "    StructField(\"humidity\", IntegerType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV with permissive mode to allow corrupt records and put them in _corrupt_record\n",
    "df = spark.read.option(\"mode\", \"PERMISSIVE\").schema(schema).csv(\"/data/*.csv\")\n",
    "\n",
    "# Add all corrupt records to a separate dataframe\n",
    "bad_df = df.filter(df[\"_corrupt_record\"].isNotNull())\n",
    "\n",
    "# Use exceptAll to ensure no overlap between valid and invalid data\n",
    "# exceptAll guarantees no duplicates even when adding more bad data checks\n",
    "df = df.exceptAll(bad_df)\n",
    "\n",
    "# Write valid and invalid data to separate tables\n",
    "# df.write.format(\"parquet\").save(\"/path/to/table\")\n",
    "# bad_df.write.format(\"parquet\").save(\"/path/to/table_bad\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
