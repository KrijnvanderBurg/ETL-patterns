{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Data Validation and Processing\n",
    "\n",
    "This notebook demonstrates how to validate CSV data using PySpark, separating valid records from invalid ones. We'll work with a simple schema and handle corrupt records gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Schema Definition\n",
    "\n",
    "First, we'll import the necessary libraries and define our schema with a special `_corrupt_record` field that captures malformed rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 08:07:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.Builder().appName(\"Handle Bad Data\").getOrCreate()\n",
    "\n",
    "# Define schema for validation (includes _corrupt_record field to capture malformed rows)\n",
    "csv_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)  # Captures rows that don't match schema\n",
    "])\n",
    "\n",
    "# Output locations\n",
    "valid_output_path = \"valid_records\"\n",
    "invalid_output_path = \"invalid_records\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Next, we'll create RDDs with test data. We have two sources:\n",
    "1. New data with some valid and invalid records\n",
    "2. Previously invalid records that we're attempting to reprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDDs for test data\n",
    "# In production, you would read from files instead of using test data\n",
    "new_data_rdd = spark.sparkContext.parallelize([\n",
    "    \"1,John Doe,30\",\n",
    "    \"2,Jane Smith,25\",\n",
    "    \"3,Bob Johnson,35\",\n",
    "    \"4,Alice Johnson,twenty\"  # Invalid: age is not an integer\n",
    "])\n",
    "\n",
    "# Previously invalid records to reprocess\n",
    "previous_invalid_rdd = spark.sparkContext.parallelize([\n",
    "    \"5,45,Charlie Brown\",  # Invalid: id and age are position swapped\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading with Schema Validation\n",
    "\n",
    "We'll load the data with our schema validation. The schema will automatically detect records that don't match the expected structure and place them in the `_corrupt_record` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All records:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----+--------------------+\n",
      "| id|         name| age|     _corrupt_record|\n",
      "+---+-------------+----+--------------------+\n",
      "|  1|     John Doe|  30|                NULL|\n",
      "|  2|   Jane Smith|  25|                NULL|\n",
      "|  3|  Bob Johnson|  35|                NULL|\n",
      "|  4|Alice Johnson|NULL|4,Alice Johnson,t...|\n",
      "|  5|           45|NULL|  5,45,Charlie Brown|\n",
      "+---+-------------+----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read data with schema validation\n",
    "new_df = spark.read.schema(csv_schema).csv(new_data_rdd)\n",
    "previous_invalid_df = spark.read.schema(csv_schema).csv(previous_invalid_rdd)\n",
    "\n",
    "# Combine datasets for unified processing\n",
    "combined_df = new_df.union(previous_invalid_df)\n",
    "\n",
    "# Show all data including corrupt records\n",
    "print(\"All records:\")\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Now we'll separate the valid records from the invalid ones using the `_corrupt_record` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate valid from invalid records\n",
    "valid_df = combined_df.filter(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\")\n",
    "invalid_df = combined_df.filter(col(\"_corrupt_record\").isNotNull()).select(\"_corrupt_record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Results\n",
    "\n",
    "We'll write the valid records as CSV with headers, and the invalid records in their original form as text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write valid records as CSV with headers\n",
    "valid_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(valid_output_path)\n",
    "\n",
    "# Write invalid records in their original form as text\n",
    "# Using text format because _corrupt_record contains the original CSV line as string\n",
    "invalid_df.write.option(\"header\", \"false\").mode(\"overwrite\").text(invalid_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Display\n",
    "\n",
    "Finally, let's display our valid and invalid records to verify the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid records:\n",
      "+---+-----------+---+\n",
      "| id|       name|age|\n",
      "+---+-----------+---+\n",
      "|  1|   John Doe| 30|\n",
      "|  2| Jane Smith| 25|\n",
      "|  3|Bob Johnson| 35|\n",
      "+---+-----------+---+\n",
      "\n",
      "Invalid records (original form):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     _corrupt_record|\n",
      "+--------------------+\n",
      "|4,Alice Johnson,t...|\n",
      "|  5,45,Charlie Brown|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "print(\"Valid records:\")\n",
    "valid_df.show()\n",
    "\n",
    "print(\"Invalid records (original form):\")\n",
    "invalid_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
